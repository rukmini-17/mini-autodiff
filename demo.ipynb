{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Reverse-Mode Automatic Differentiation Engine\n",
        "**Author:** Rukmini Nazre\n",
        "\n",
        "This notebook demonstrates a custom-built Automatic Differentiation (AD) engine implemented from scratch in Python and NumPy. It supports reverse-mode differentiation (backpropagation) through a dynamic computational graph, similar to PyTorch's `autograd`.\n",
        "\n",
        "**Key Features:**\n",
        "* Dynamic graph construction using `Var` objects.\n",
        "* Support for scalar and tensor operations (`matmul`, `solve`, `logdet`, etc.).\n",
        "* Numerical validation against standard finite-difference approximations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import autodiff as ad  # import custom library\n",
        "from autodiff import Var \n",
        "\n",
        "# random seed for reproducibility\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Demo 1: Simple Scalar Arithmetic\n",
        "First, we validate the engine on a simple function:\n",
        "$$z = x \\cdot y + x$$\n",
        "\n",
        "We expect the gradients to be:\n",
        "* $\\frac{\\partial z}{\\partial x} = y + 1$\n",
        "* $\\frac{\\partial z}{\\partial y} = x$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inputs: x=2.0, y=3.0\n",
            "Computed Gradient dx: 4.0 (Expected: 4.0)\n",
            "Computed Gradient dy: 2.0 (Expected: 2.0)\n"
          ]
        }
      ],
      "source": [
        "# 1. define inputs\n",
        "x_val = 2.0\n",
        "y_val = 3.0\n",
        "\n",
        "# 2. define the computation function\n",
        "def simple_math(x, y):\n",
        "    # z = x * y + x\n",
        "    return x * y + x \n",
        "\n",
        "# 3. compute gradients using the autodiff engine\n",
        "# ad.grad takes a function and returns a new function that outputs gradients\n",
        "grad_fn = ad.grad(simple_math)\n",
        "grads = grad_fn(x_val, y_val)\n",
        "\n",
        "print(f\"Inputs: x={x_val}, y={y_val}\")\n",
        "print(f\"Computed Gradient dx: {grads[0]} (Expected: {y_val + 1})\")\n",
        "print(f\"Computed Gradient dy: {grads[1]} (Expected: {x_val})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Demo 2: Multivariate Gaussian Log-Likelihood\n",
        "This example demonstrates the engine's ability to handle complex Linear Algebra operations. We compute the gradients for the **Negative Log-Likelihood** of a Multivariate Gaussian distribution with respect to the covariance matrix $\\Sigma$.\n",
        "\n",
        "The operations involved include:\n",
        "* Matrix Multiplication (`@`)\n",
        "* Linear Solve (`np.linalg.solve`)\n",
        "* Log Determinant (`np.linalg.slogdet`)\n",
        "* Summation and Trace operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data Shape: (100, 5)\n",
            "Covariance Matrix Shape: (5, 5)\n"
          ]
        }
      ],
      "source": [
        "# data dimensions\n",
        "N = 100  # number of samples\n",
        "D = 5    # dimension of features\n",
        "\n",
        "# generate synthetic data (centering data to mean 0 for simplicity)\n",
        "X_np = np.random.randn(N, D)\n",
        "\n",
        "# generate a valid positive-definite covariance matrix Sigma\n",
        "A = np.random.randn(D, D)\n",
        "Sigma_np = A @ A.T + 0.1 * np.eye(D)  # ensure invertibility\n",
        "\n",
        "print(f\"Data Shape: {X_np.shape}\")\n",
        "print(f\"Covariance Matrix Shape: {Sigma_np.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 1. Define the Loss Function using AutoDiff Operators ---\n",
        "def gaussian_log_likelihood_loss(Sigma):\n",
        "    # Loss = (N/2)*logdet(Sigma) + 0.5 * sum( x.T @ inv(Sigma) @ x )\n",
        "    \n",
        "    # Term 1: Log Determinant\n",
        "    term1 = ad.mul(ad.Var(N / 2), ad.logdet(Sigma))\n",
        "    \n",
        "    # Term 2: Mahalanobis Distance (using solve for stability instead of explicit inv)\n",
        "    # We want sum(X_i^T @ Sigma^-1 @ X_i). \n",
        "    # Efficient trick: sum(X * solve(Sigma, X.T).T) equivalent to trace/sum logic\n",
        "    \n",
        "    # Compute Sigma^-1 @ X.T (Shape: D x N)\n",
        "    X_var = ad.Var(X_np.T) # Transpose to (D, N) for solve\n",
        "    inv_sigma_x = ad.solve(Sigma, X_var) \n",
        "    \n",
        "    # Compute quadratic term: 0.5 * sum(X_var * inv_sigma_x) \n",
        "    # Element-wise multiply and sum acts like the trace of the product\n",
        "    product = X_var * inv_sigma_x\n",
        "    term2 = ad.mul(ad.Var(0.5), ad._sum(product))\n",
        "    \n",
        "    return term1 + term2\n",
        "\n",
        "# --- 2. Numerical Gradient Helper (Finite Difference) ---\n",
        "def numerical_gradient(f, x, eps=1e-6):\n",
        "    grad = np.zeros_like(x)\n",
        "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
        "    \n",
        "    while not it.finished:\n",
        "        idx = it.multi_index\n",
        "        orig_val = x[idx]\n",
        "        \n",
        "        # f(x + eps)\n",
        "        x[idx] = orig_val + eps\n",
        "        f_plus = f(x)\n",
        "        \n",
        "        # f(x - eps)\n",
        "        x[idx] = orig_val - eps\n",
        "        f_minus = f(x)\n",
        "        \n",
        "        # Finite difference\n",
        "        grad[idx] = (f_plus - f_minus) / (2 * eps)\n",
        "        \n",
        "        x[idx] = orig_val # Restore\n",
        "        it.iternext()\n",
        "        \n",
        "    return grad\n",
        "\n",
        "# Wrapper for numerical check (only needs numpy ops)\n",
        "def numpy_loss_wrapper(Sigma_val):\n",
        "    sign, logdet = np.linalg.slogdet(Sigma_val)\n",
        "    term1 = (N / 2) * logdet\n",
        "    \n",
        "    # Manual inverse for numpy check\n",
        "    inv_sigma = np.linalg.inv(Sigma_val)\n",
        "    # term2 = 0.5 * sum( (x @ inv_sigma @ x.T) )\n",
        "    term2 = 0.5 * np.sum(X_np @ inv_sigma * X_np) \n",
        "    \n",
        "    return term1 + term2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Backpropagation...\n",
            "Computing Numerical Gradient (this may take a moment)...\n",
            "------------------------------\n",
            "Gradient Norm (AutoDiff): 134.882423\n",
            "Gradient Norm (Numerical): 134.882423\n",
            "Relative Error: 4.01e-10\n",
            "------------------------------\n",
            "SUCCESS: AutoDiff gradients match numerical approximation!\n"
          ]
        }
      ],
      "source": [
        "print(\"Running Backpropagation...\")\n",
        "\n",
        "# 1. Compute Gradient using AutoDiff Engine\n",
        "grad_fn = ad.grad(gaussian_log_likelihood_loss)\n",
        "autodiff_grads = grad_fn(Sigma_np)\n",
        "grad_sigma_autodiff = autodiff_grads[0]\n",
        "\n",
        "# 2. Compute Numerical Gradient for Verification\n",
        "print(\"Computing Numerical Gradient (this may take a moment)...\")\n",
        "grad_sigma_numerical = numerical_gradient(numpy_loss_wrapper, Sigma_np.copy())\n",
        "\n",
        "# 3. Compare Results\n",
        "diff = np.linalg.norm(grad_sigma_autodiff - grad_sigma_numerical)\n",
        "denom = np.linalg.norm(grad_sigma_autodiff) + np.linalg.norm(grad_sigma_numerical)\n",
        "rel_error = diff / denom\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(f\"Gradient Norm (AutoDiff): {np.linalg.norm(grad_sigma_autodiff):.6f}\")\n",
        "print(f\"Gradient Norm (Numerical): {np.linalg.norm(grad_sigma_numerical):.6f}\")\n",
        "print(f\"Relative Error: {rel_error:.2e}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "if rel_error < 1e-5:\n",
        "    print(\"SUCCESS: AutoDiff gradients match numerical approximation!\")\n",
        "else:\n",
        "    print(\"WARNING: Gradients differ. Check implementation.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
